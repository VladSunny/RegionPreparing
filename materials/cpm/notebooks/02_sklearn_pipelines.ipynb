{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "–ü–∞–π–ø–ª–∞–π–Ω—ã sklearn: Pipeline –∏ ColumnTransformer ‚Äî –ø—Ä–∞–∫—Ç–∏–∫—É–º\n",
    "\n",
    "**–ö–∞–∫ —Ä–∞–±–æ—Ç–∞—Ç—å:**\n",
    "- –°–Ω–∞—á–∞–ª–∞ –≤—Å–ø–æ–º–∏–Ω–∞–µ–º –∏–¥–µ—é –ø–∞–π–ø–ª–∞–π–Ω–æ–≤, –∑–∞—Ç–µ–º —Å—Ä–∞–∑—É —Å–æ–±–∏—Ä–∞–µ–º —Å–≤–æ–π.\n",
    "- –ü–æ—Å–ª–µ –∫–∞–∂–¥–æ–≥–æ —à–∞–≥–∞ –æ—Ç–≤–µ—á–∞–µ–º –Ω–∞ –≤–æ–ø—Ä–æ—Å ¬´–∑–∞—á–µ–º —ç—Ç–æ –≤ –ø—Ä–æ–¥–∞–∫—à–µ–Ω–µ?¬ª.\n",
    "- –ü—Ä–∏–º–µ—Ä—ã –∏–∑ `boostings_101.ipynb` –∏ `eda-feat-engeen.ipynb` –º–æ–∂–Ω–æ –ø–æ–¥—Å–º–∞—Ç—Ä–∏–≤–∞—Ç—å, –Ω–æ –∫–æ–¥ –ø–∏—à–µ–º —Å–∞–º–∏.\n",
    "\n",
    "**–ö–ª—é—á–µ–≤—ã–µ –Ω–∞–≤—ã–∫–∏:**\n",
    "- –°–æ–±–∏—Ä–∞—Ç—å —Ü–µ–ø–æ—á–∫–∏ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞ –∏ –º–æ–¥–µ–ª–∏ –±–µ–∑ —É—Ç–µ—á–µ–∫.\n",
    "- –†–∞–∑–Ω–æ—Å–∏—Ç—å —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ —á–µ—Ä–µ–∑ ColumnTransformer.\n",
    "- –ü—Ä–æ–≤–µ—Ä—è—Ç—å, —á—Ç–æ –ø–∞–π–ø–ª–∞–π–Ω —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ –Ω–∞ –Ω–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.datasets import make_classification, fetch_openml\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline, make_pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score, classification_report\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏—è\n",
    "1. –ú–∏–Ω–∏-–±–∞–∑–ª–∞–π–Ω –±–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω–∞ (—Ç–æ–ª—å–∫–æ fit/predict).\n",
    "2. –ü–µ—Ä–≤—ã–π Pipeline —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –º–æ–¥–µ–ª—å—é.\n",
    "3. ColumnTransformer: —Ä–∞–∑–¥–µ–ª—è–µ–º —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤.\n",
    "4. –í–∞–ª–∏–¥–∞—Ü–∏—è: –ø—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –ø–∞–π–ø–ª–∞–π–Ω –Ω–µ —Ç–µ—á—ë—Ç –≤ cross_val_score.\n",
    "5. –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç: –¥–æ–±–∞–≤—å—Ç–µ —Å–≤–æ–π —à–∞–≥ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–¥–∫–∏—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π) –∏ –æ—Ü–µ–Ω–∏—Ç–µ —ç—Ñ—Ñ–µ–∫—Ç.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. –ó–∞—á–µ–º –Ω—É–∂–Ω—ã –ø–∞–π–ø–ª–∞–π–Ω—ã?\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞ –±–µ–∑ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤:**\n",
    "\n",
    "```python\n",
    "# ‚ùå –¢–∞–∫ –¥–µ–ª–∞—Ç—å –ù–ï–õ–¨–ó–Ø ‚Äî —É—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö!\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # fit –Ω–∞ –í–°–ï–• –¥–∞–Ω–Ω—ã—Ö\n",
    "X_train, X_test = train_test_split(X_scaled, ...)\n",
    "```\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º—ã:**\n",
    "1. –£—Ç–µ—á–∫–∞ –¥–∞–Ω–Ω—ã—Ö (scaler –≤–∏–¥–∏—Ç test –¥–∞–Ω–Ω—ã–µ)\n",
    "2. –ö–æ–¥ –Ω–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º (–º–Ω–æ–≥–æ —Ä—É—á–Ω—ã—Ö —à–∞–≥–æ–≤)\n",
    "3. –õ–µ–≥–∫–æ –∑–∞–±—ã—Ç—å –ø—Ä–∏–º–µ–Ω–∏—Ç—å transform –∫ –Ω–æ–≤—ã–º –¥–∞–Ω–Ω—ã–º\n",
    "\n",
    "**–†–µ—à–µ–Ω–∏–µ ‚Äî Pipeline:**\n",
    "\n",
    "```python\n",
    "# ‚úÖ –¢–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ\n",
    "pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('model', LogisticRegression())\n",
    "])\n",
    "pipe.fit(X_train, y_train)  # scaler.fit —Ç–æ–ª—å–∫–æ –Ω–∞ train!\n",
    "pipe.predict(X_test)  # –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –ø—Ä–∏–º–µ–Ω—è–µ—Ç transform\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Pipeline: –±–∞–∑–æ–≤—ã–π –ø—Ä–∏–º–µ—Ä\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ\n",
    "X, y = make_classification(\n",
    "    n_samples=1000, n_features=20, n_informative=10,\n",
    "    n_classes=2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {X_train.shape}, Test: {X_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º –ø—Ä–æ—Å—Ç–æ–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "simple_pipe = Pipeline([\n",
    "    ('scaler', StandardScaler()),           # –®–∞–≥ 1: —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è\n",
    "    ('classifier', LogisticRegression())    # –®–∞–≥ 2: –º–æ–¥–µ–ª—å\n",
    "])\n",
    "\n",
    "# –û–±—É—á–∞–µ–º ‚Äî scaler.fit_transform –Ω–∞ train, –ø–æ—Ç–æ–º model.fit\n",
    "simple_pipe.fit(X_train, y_train)\n",
    "\n",
    "# –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º ‚Äî scaler.transform –Ω–∞ test, –ø–æ—Ç–æ–º model.predict\n",
    "y_pred = simple_pipe.predict(X_test)\n",
    "y_pred_proba = simple_pipe.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—ã–π —Å–∏–Ω—Ç–∞–∫—Å–∏—Å ‚Äî make_pipeline:**\n",
    "\n",
    "–ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –Ω–∞–∑—ã–≤–∞–µ—Ç —à–∞–≥–∏ –ø–æ –∏–º–µ–Ω–∞–º –∫–ª–∞—Å—Å–æ–≤ (–≤ –Ω–∏–∂–Ω–µ–º —Ä–µ–≥–∏—Å—Ç—Ä–µ).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –¢–æ –∂–µ —Å–∞–º–æ–µ, –Ω–æ –∫–æ—Ä–æ—á–µ\n",
    "simple_pipe_v2 = make_pipeline(StandardScaler(), LogisticRegression())\n",
    "\n",
    "# –°–º–æ—Ç—Ä–∏–º –Ω–∞ —à–∞–≥–∏\n",
    "print(\"–®–∞–≥–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞:\")\n",
    "for name, step in simple_pipe_v2.named_steps.items():\n",
    "    print(f\"  {name}: {type(step).__name__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. ColumnTransformer: –æ–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–Ω—ã—Ö —Ç–∏–ø–æ–≤ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "–í —Ä–µ–∞–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –µ—Å—Ç—å —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏. –ò–º –Ω—É–∂–Ω–∞ —Ä–∞–∑–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞!\n",
    "\n",
    "**ColumnTransformer** –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø—Ä–∏–º–µ–Ω—è—Ç—å —Ä–∞–∑–Ω—ã–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∞—Ü–∏–∏ –∫ —Ä–∞–∑–Ω—ã–º –∫–æ–ª–æ–Ω–∫–∞–º.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∞–∫—Ç–∏–∫–∞: —Å–æ–±–µ—Ä–∏—Ç–µ –ø–µ—Ä–≤—ã–π —Ä–∞–±–æ—á–∏–π Pipeline\n",
    "–ó–∞–ø–æ–ª–Ω–∏—Ç–µ TODO: —Å–æ–∑–¥–∞–π—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω —Å –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ–º –∏ –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–µ–π.\n",
    "–û–±—è–∑–∞—Ç–µ–ª—å–Ω–æ —É–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ scaler –æ–±—É—á–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ train –≤–Ω—É—Ç—Ä–∏ CV.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: –≤–∞—à –ø–µ—Ä–≤—ã–π Pipeline\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.model_selection import cross_val_score\n",
    "# pipeline = Pipeline([\n",
    "#     (\"scaler\", StandardScaler()),\n",
    "#     (\"model\", LogisticRegression(max_iter=500))\n",
    "# ])\n",
    "# scores = cross_val_score(pipeline, X, y, cv=5)\n",
    "# print(\"CV accuracy:\", scores)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º –¥–∞—Ç–∞—Å–µ—Ç —Å —Ä–∞–∑–Ω—ã–º–∏ —Ç–∏–ø–∞–º–∏ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "np.random.seed(RANDOM_STATE)\n",
    "n = 500\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'age': np.random.randint(18, 70, n),\n",
    "    'income': np.random.randint(20000, 200000, n),\n",
    "    'credit_score': np.random.randint(300, 850, n),\n",
    "    'education': np.random.choice(['high_school', 'bachelor', 'master', 'phd'], n),\n",
    "    'employment': np.random.choice(['employed', 'self_employed', 'unemployed'], n),\n",
    "    'city': np.random.choice(['Moscow', 'SPb', 'Kazan', 'Novosibirsk', 'Ekaterinburg'], n),\n",
    "})\n",
    "\n",
    "# –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏\n",
    "df.loc[np.random.choice(n, 30, replace=False), 'income'] = np.nan\n",
    "df.loc[np.random.choice(n, 20, replace=False), 'education'] = np.nan\n",
    "\n",
    "# –¢–∞—Ä–≥–µ—Ç (–±–∏–Ω–∞—Ä–Ω–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è)\n",
    "y = (df['income'].fillna(df['income'].median()) > 80000).astype(int)\n",
    "\n",
    "print(\"–ü—Ä–∏–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö:\")\n",
    "print(df.head(10))\n",
    "print(f\"\\n–ü—Ä–æ–ø—É—Å–∫–∏:\\n{df.isnull().sum()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–∏–ø—ã –∫–æ–ª–æ–Ω–æ–∫\n",
    "numeric_features = ['age', 'income', 'credit_score']\n",
    "categorical_features = ['education', 'employment', 'city']\n",
    "\n",
    "print(f\"–ß–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {numeric_features}\")\n",
    "print(f\"–ö–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏: {categorical_features}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º –ø–∞–π–ø–ª–∞–π–Ω—ã –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–∏–ø–∞ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "# –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö: –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–µ–¥–∏–∞–Ω–æ–π + —Å—Ç–∞–Ω–¥–∞—Ä—Ç–∏–∑–∞—Ü–∏—è\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: –∑–∞–ø–æ–ª–Ω—è–µ–º –ø—Ä–æ–ø—É—Å–∫–∏ –º–æ–¥–æ–π + OneHot encoding\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# –û–±—ä–µ–¥–∏–Ω—è–µ–º –≤—Å—ë –≤ ColumnTransformer\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Preprocessor —Å–æ–∑–¥–∞–Ω!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω: –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ + –º–æ–¥–µ–ª—å\n",
    "full_pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# –†–∞–∑–±–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df, y, test_size=0.2, random_state=RANDOM_STATE, stratify=y\n",
    ")\n",
    "\n",
    "# –û–±—É—á–∞–µ–º\n",
    "full_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# –û—Ü–µ–Ω–∏–≤–∞–µ–º\n",
    "y_pred_proba = full_pipeline.predict_proba(X_test)[:, 1]\n",
    "print(f\"ROC-AUC: {roc_auc_score(y_test, y_pred_proba):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–º–æ—Ç—Ä–∏–º, –∫–∞–∫–∏–µ —Ñ–∏—á–∏ –ø–æ–ª—É—á–∏–ª–∏—Å—å –ø–æ—Å–ª–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# –ü–æ–ª—É—á–∞–µ–º –∏–º–µ–Ω–∞ —Ñ–∏—á–µ–π\n",
    "cat_encoder = preprocessor.named_transformers_['cat'].named_steps['onehot']\n",
    "cat_feature_names = cat_encoder.get_feature_names_out(categorical_features)\n",
    "\n",
    "all_feature_names = list(numeric_features) + list(cat_feature_names)\n",
    "print(f\"–í—Å–µ–≥–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –ø–æ—Å–ª–µ –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥–∞: {len(all_feature_names)}\")\n",
    "print(f\"\\n–ü—Ä–∏–∑–Ω–∞–∫–∏: {all_feature_names}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. –û–±—Ä–∞–±–æ—Ç–∫–∞ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤\n",
    "\n",
    "–°—É—â–µ—Å—Ç–≤—É–µ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ —Å–ø–æ—Å–æ–±–æ–≤ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π:\n",
    "\n",
    "| –ú–µ—Ç–æ–¥ | –ö–æ–≥–¥–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å | –ü–ª—é—Å—ã | –ú–∏–Ω—É—Å—ã |\n",
    "|-------|-------------------|-------|--------|\n",
    "| **OneHotEncoder** | –ú–∞–ª–æ —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π | –ù–µ –≤–Ω–æ—Å–∏—Ç –ø–æ—Ä—è–¥–æ–∫ | –ú–Ω–æ–≥–æ –∫–æ–ª–æ–Ω–æ–∫ |\n",
    "| **OrdinalEncoder** | –ï—Å—Ç—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ | –ö–æ–º–ø–∞–∫—Ç–Ω–æ | –í–Ω–æ—Å–∏—Ç –ª–æ–∂–Ω—ã–π –ø–æ—Ä—è–¥–æ–∫ |\n",
    "| **TargetEncoder** | –ú–Ω–æ–≥–æ –∫–∞—Ç–µ–≥–æ—Ä–∏–π, –¥–µ—Ä–µ–≤—å—è | –ö–æ–º–ø–∞–∫—Ç–Ω–æ, –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ | –†–∏—Å–∫ —É—Ç–µ—á–∫–∏! |\n",
    "\n",
    "### 4.1 OneHotEncoder (–¥–µ—Ñ–æ–ª—Ç–Ω—ã–π –≤—ã–±–æ—Ä)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ü—Ä–∞–∫—Ç–∏–∫–∞: ColumnTransformer –ø–æ–¥ –≤–∞—à –¥–∞—Ç–∞—Å–µ—Ç\n",
    "- –†–∞–∑–¥–µ–ª–∏—Ç–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞ —á–∏—Å–ª–æ–≤—ã–µ –∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ.\n",
    "- –î–æ–±–∞–≤—å—Ç–µ —Ä–∞–∑–Ω—ã–µ –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è (–Ω–∞–ø—Ä–∏–º–µ—Ä, StandardScaler –¥–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –∏ OneHotEncoder –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö).\n",
    "- –°—Ä–∞–≤–Ω–∏—Ç–µ –º–µ—Ç—Ä–∏–∫–∏ –¥–æ/–ø–æ—Å–ª–µ —Ä–∞–∑–¥–µ–ª–µ–Ω–∏—è.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: –Ω–∞—Å—Ç—Ä–æ–π—Ç–µ ColumnTransformer –∏ –æ–±–µ—Ä–Ω–∏—Ç–µ –≤ Pipeline\n",
    "# from sklearn.compose import ColumnTransformer\n",
    "# from sklearn.preprocessing import OneHotEncoder\n",
    "# numeric_features = [...]\n",
    "# categorical_features = [...]\n",
    "# preprocessor = ColumnTransformer(\n",
    "#     [\n",
    "#         (\"num\", StandardScaler(), numeric_features),\n",
    "#         (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), categorical_features)\n",
    "#     ]\n",
    "# )\n",
    "# model = ...\n",
    "# full_pipeline = Pipeline([\n",
    "#     (\"preprocess\", preprocessor),\n",
    "#     (\"model\", model)\n",
    "# ])\n",
    "# scores = cross_val_score(full_pipeline, X, y, cv=5, scoring=\"roc_auc\")\n",
    "# print(scores)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä OneHotEncoder\n",
    "sample_data = pd.DataFrame({\n",
    "    'color': ['red', 'blue', 'green', 'red', 'blue']\n",
    "})\n",
    "\n",
    "ohe = OneHotEncoder(sparse_output=False)\n",
    "encoded = ohe.fit_transform(sample_data)\n",
    "\n",
    "print(\"–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\")\n",
    "print(sample_data['color'].values)\n",
    "print(\"\\n–ü–æ—Å–ª–µ OneHot:\")\n",
    "print(pd.DataFrame(encoded, columns=ohe.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 OrdinalEncoder (–¥–ª—è —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä OrdinalEncoder –¥–ª—è —É–ø–æ—Ä—è–¥–æ—á–µ–Ω–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "sample_data = pd.DataFrame({\n",
    "    'size': ['S', 'M', 'L', 'XL', 'M', 'S']\n",
    "})\n",
    "\n",
    "# –ó–∞–¥–∞—ë–º –ø–æ—Ä—è–¥–æ–∫ –∫–∞—Ç–µ–≥–æ—Ä–∏–π —è–≤–Ω–æ!\n",
    "oe = OrdinalEncoder(categories=[['S', 'M', 'L', 'XL']])\n",
    "encoded = oe.fit_transform(sample_data)\n",
    "\n",
    "print(\"–ò—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ:\")\n",
    "print(sample_data['size'].values)\n",
    "print(\"\\n–ü–æ—Å–ª–µ Ordinal (—Å –∑–∞–¥–∞–Ω–Ω—ã–º –ø–æ—Ä—è–¥–∫–æ–º):\")\n",
    "print(encoded.flatten())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Target Encoding (–æ—Å—Ç–æ—Ä–æ–∂–Ω–æ ‚Äî —Ä–∏—Å–∫ —É—Ç–µ—á–∫–∏!)\n",
    "\n",
    "**Target Encoding** –∑–∞–º–µ–Ω—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é –Ω–∞ —Å—Ä–µ–¥–Ω–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ —Ç–∞—Ä–≥–µ—Ç–∞ –¥–ª—è —ç—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–∏.\n",
    "\n",
    "**–ü—Ä–æ–±–ª–µ–º–∞:** –ï—Å–ª–∏ –¥–µ–ª–∞—Ç—å –Ω–∞ –≤—Å–µ—Ö –¥–∞–Ω–Ω—ã—Ö ‚Äî —É—Ç–µ—á–∫–∞! –°—Ä–µ–¥–Ω–µ–µ —Ç–∞—Ä–≥–µ—Ç–∞ –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ ¬´–ø–æ–¥–≥–ª—è–¥—ã–≤–∞–µ—Ç¬ª –≤ test.\n",
    "\n",
    "**–ë–µ–∑–æ–ø–∞—Å–Ω—ã–π —Å–ø–æ—Å–æ–±:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å `TargetEncoder` –∏–∑ sklearn (–≤–µ—Ä—Å–∏—è 1.3+), –∫–æ—Ç–æ—Ä—ã–π –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ –¥–µ–ª–∞–µ—Ç CV –≤–Ω—É—Ç—Ä–∏.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –ü—Ä–∏–º–µ—Ä Target Encoding (sklearn >= 1.3)\n",
    "try:\n",
    "    from sklearn.preprocessing import TargetEncoder\n",
    "    \n",
    "    # –î–∞–Ω–Ω—ã–µ —Å –≤—ã—Å–æ–∫–æ–π –∫–∞—Ä–¥–∏–Ω–∞–ª—å–Ω–æ—Å—Ç—å—é\n",
    "    sample_X = pd.DataFrame({\n",
    "        'city': ['Moscow', 'SPb', 'Kazan', 'Moscow', 'SPb', 'Novosibirsk', \n",
    "                 'Moscow', 'Kazan', 'SPb', 'Moscow']\n",
    "    })\n",
    "    sample_y = np.array([1, 0, 1, 1, 0, 0, 1, 1, 0, 1])\n",
    "    \n",
    "    te = TargetEncoder(smooth='auto', random_state=RANDOM_STATE)\n",
    "    encoded = te.fit_transform(sample_X, sample_y)\n",
    "    \n",
    "    print(\"Target Encoding:\")\n",
    "    print(pd.DataFrame({\n",
    "        'city': sample_X['city'],\n",
    "        'target': sample_y,\n",
    "        'encoded': encoded.flatten()\n",
    "    }))\\n    \n",
    "except ImportError:\n",
    "    print(\"TargetEncoder –¥–æ—Å—Ç—É–ø–µ–Ω –≤ sklearn >= 1.3\")\n",
    "    print(\"–î–ª—è –±–æ–ª–µ–µ —Ä–∞–Ω–Ω–∏—Ö –≤–µ—Ä—Å–∏–π –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ category_encoders.TargetEncoder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω —Å –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\n",
    "\n",
    "–ì–ª–∞–≤–Ω–æ–µ –ø—Ä–µ–∏–º—É—â–µ—Å—Ç–≤–æ –ø–∞–π–ø–ª–∞–π–Ω–æ–≤ ‚Äî –∏—Ö –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∞—Ç—å –≤ `cross_val_score`, –∏ –≤—Å—ë –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### –ú–∏–Ω–∏-–ø—Ä–æ–µ–∫—Ç: –¥–æ–≤–µ–¥–∏—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω –¥–æ –ø—Ä–æ–¥–∞–∫—à–Ω-—Ñ–æ—Ä–º–∞—Ç–∞\n",
    "–°—Ñ–æ—Ä–º—É–ª–∏—Ä—É–π—Ç–µ –∑–∞–¥–∞—á—É (–∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è –∏–ª–∏ —Ä–µ–≥—Ä–µ—Å—Å–∏—è) –∏ —Å–æ–±–µ—Ä–∏—Ç–µ –ø–∞–π–ø–ª–∞–π–Ω —Å —Ç—Ä–µ–º—è —Å–ª–æ—è–º–∏: –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ ‚Üí –º–æ–¥–µ–ª—å ‚Üí –ø–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞.\n",
    "- –î–æ–±–∞–≤—å—Ç–µ GridSearchCV/RandomizedSearchCV –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤.\n",
    "- –ü—Ä–æ–≤–µ—Ä—å—Ç–µ —É—Å—Ç–æ–π—á–∏–≤–æ—Å—Ç—å —á–µ—Ä–µ–∑ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é.\n",
    "- –°–æ—Ö—Ä–∞–Ω–∏—Ç–µ –∫–ª—é—á–µ–≤—ã–µ –º–µ—Ç—Ä–∏–∫–∏ –∏ –∫–æ—Ä–æ—Ç–∫–∏–π –≤—ã–≤–æ–¥.\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "# TODO: –≤–∞—à —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞–ª—å–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# search = GridSearchCV(full_pipeline, {...}, cv=3, scoring=\"roc_auc\")\n",
    "# search.fit(X, y)\n",
    "# print(\"Best params:\", search.best_params_)\n",
    "# print(\"Best score:\", search.best_score_)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# –°–æ–∑–¥–∞—ë–º –ø–∞–π–ø–ª–∞–π–Ω –∑–∞–Ω–æ–≤–æ –¥–ª—è —á–∏—Å—Ç–æ—Ç—ã\n",
    "numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num', numeric_transformer, numeric_features),\n",
    "    ('cat', categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "# –ü–æ–ª–Ω—ã–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è ‚Äî –ø—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ fit'–∏—Ç—Å—è –Ω–∞ –∫–∞–∂–¥–æ–º train —Ñ–æ–ª–¥–µ!\n",
    "cv_scores = cross_val_score(\n",
    "    pipeline, df, y, \n",
    "    cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE),\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "print(f\"CV ROC-AUC: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f}\")\n",
    "print(f\"Scores: {cv_scores}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**–ü–æ—á–µ–º—É —ç—Ç–æ —Ä–∞–±–æ—Ç–∞–µ—Ç –ø—Ä–∞–≤–∏–ª—å–Ω–æ:**\n",
    "\n",
    "–ù–∞ –∫–∞–∂–¥–æ–º —Ñ–æ–ª–¥–µ:\n",
    "1. `preprocessor.fit()` –≤—ã–∑—ã–≤–∞–µ—Ç—Å—è —Ç–æ–ª—å–∫–æ –Ω–∞ train —á–∞—Å—Ç–∏\n",
    "2. `preprocessor.transform()` –ø—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è –∫ val —á–∞—Å—Ç–∏\n",
    "3. –ú–æ–¥–µ–ª—å –æ–±—É—á–∞–µ—Ç—Å—è –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–º train\n",
    "4. –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–µ–ª–∞–µ—Ç—Å—è –Ω–∞ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–Ω–æ–º val\n",
    "\n",
    "–ù–∏–∫–∞–∫–∏—Ö —É—Ç–µ—á–µ–∫! üéâ\n",
    "\n",
    "---\n",
    "## 6. –ü—Ä–∞–∫—Ç–∏–∫–∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: –°–æ–∑–¥–∞–π—Ç–µ —Å–≤–æ–π –ø–∞–π–ø–ª–∞–π–Ω\n",
    "# 1. –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ —Å—Ä–µ–¥–Ω–∏–º, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–π—Ç–µ MinMaxScaler\n",
    "# 2. –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö: –∑–∞–ø–æ–ª–Ω–∏—Ç–µ –ø—Ä–æ–ø—É—Å–∫–∏ –∫–æ–Ω—Å—Ç–∞–Ω—Ç–æ–π 'missing', OneHotEncoder\n",
    "# 3. –ú–æ–¥–µ–ª—å: LogisticRegression\n",
    "# 4. –û—Ü–µ–Ω–∏—Ç–µ –∫–∞—á–µ—Å—Ç–≤–æ —á–µ—Ä–µ–∑ cross_val_score\n",
    "\n",
    "# –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ —Ç–æ—Ç –∂–µ –¥–∞—Ç–∞—Å–µ—Ç df –∏ —Ç–∞—Ä–≥–µ—Ç y\n",
    "\n",
    "# –í–ê–® –ö–û–î –ó–î–ï–°–¨\n",
    "my_numeric_transformer = Pipeline([\n",
    "    # ...\n",
    "])\n",
    "\n",
    "my_categorical_transformer = Pipeline([\n",
    "    # ...\n",
    "])\n",
    "\n",
    "my_preprocessor = ColumnTransformer([\n",
    "    # ...\n",
    "])\n",
    "\n",
    "my_pipeline = Pipeline([\n",
    "    # ...\n",
    "])\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞\n",
    "# my_cv_scores = cross_val_score(my_pipeline, df, y, cv=5, scoring='roc_auc')\n",
    "# print(f\"CV ROC-AUC: {my_cv_scores.mean():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "my_numeric_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', MinMaxScaler())\n",
    "])\n",
    "\n",
    "my_categorical_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "my_preprocessor = ColumnTransformer([\n",
    "    ('num', my_numeric_transformer, numeric_features),\n",
    "    ('cat', my_categorical_transformer, categorical_features)\n",
    "])\n",
    "\n",
    "my_pipeline = Pipeline([\n",
    "    ('preprocessor', my_preprocessor),\n",
    "    ('classifier', LogisticRegression(random_state=RANDOM_STATE))\n",
    "])\n",
    "\n",
    "# –ü—Ä–æ–≤–µ—Ä–∫–∞\n",
    "my_cv_scores = cross_val_score(my_pipeline, df, y, cv=5, scoring='roc_auc')\n",
    "print(f\"CV ROC-AUC: {my_cv_scores.mean():.4f} ¬± {my_cv_scores.std():.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## –ß—Ç–æ –¥–∞–ª—å—à–µ?\n",
    "\n",
    "–¢–µ–ø–µ—Ä—å –≤—ã —É–º–µ–µ—Ç–µ:\n",
    "- –°—Ç—Ä–æ–∏—Ç—å –±–µ–∑–æ–ø–∞—Å–Ω—ã–µ –ø–∞–π–ø–ª–∞–π–Ω—ã –±–µ–∑ —É—Ç–µ—á–µ–∫ –¥–∞–Ω–Ω—ã—Ö\n",
    "- –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—Ç—å —Ä–∞–∑–Ω—ã–µ —Ç–∏–ø—ã –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ —Å ColumnTransformer\n",
    "- –í—ã–±–∏—Ä–∞—Ç—å –ø–æ–¥—Ö–æ–¥—è—â–∏–π —Å–ø–æ—Å–æ–± –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è –∫–∞—Ç–µ–≥–æ—Ä–∏–π\n",
    "\n",
    "**–°–ª–µ–¥—É—é—â–∏–π —à–∞–≥:** `03_visual_diagnostics.ipynb` ‚Äî –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –¥–ª—è EDA –∏ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –º–æ–¥–µ–ª–µ–π.\n",
    "\n",
    "**–¢–∞–∫–∂–µ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º:**\n",
    "- `01_cv_metrics_leakage.ipynb` ‚Äî –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è –∏ –º–µ—Ç—Ä–∏–∫–∏\n",
    "- `boostings_101.ipynb` ‚Äî –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ (—Ç–∞–º —Ç–æ–∂–µ –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –ø–∞–π–ø–ª–∞–π–Ω—ã!)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}